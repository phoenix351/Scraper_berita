# isi file scraper kompas

# -*- coding: utf-8 -*-
"""scraper_kompas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10b2Q7XpYpERjQzDlWI1XhTda5LVYUbPU
"""

import scrapy
from re import findall 
from datetime import datetime,timedelta
import sys
from berita.items import BeritaItem

class Okezone_spider(scrapy.Spider):
    #tanggal = "2020-3-19"
    name = "okezone_spider"
    download_delay = 0.3
    tanggal=''
    costum_settings = {
      'LOG_LEVEL':'ERROR'
    }
    
    
    hal = 0
    def __init__(self,tanggal='',*args,**kwargs):
      super(Okezone_spider, self).__init__(*args, **kwargs)
      if len(str(tanggal))<2:
        kemarin = (datetime.now() - timedelta(1))
        self.tanggal=datetime.strftime(kemarin,'%Y/%m/%d')
      else:
        tanggal = datetime.strptime(tanggal,"%d-%m-%Y")
        tanggal=datetime.strftime(tanggal,'%Y/%m/%d')
        self.tanggal=tanggal  

      
      
      self.start_urls = [('https://index.okezone.com/bydate/channel/'+self.tanggal+'/1')]

    def parse(self,response):
      konten_selektor = '#areax ul div li'
      #menghitung jumlah berita di halaman
      jumlah_berita = 0
      for konten in response.css(konten_selektor):
        #crawl on each url 
        link_selector = 'a ::attr(href)'
        link = konten.css(link_selector).extract_first()+ "?page=all"
        jumlah_berita = jumlah_berita +1
        
        req = scrapy.Request(link, callback=self.parse_artikel)
        yield req
      print("+++++++++++++++++++++++++++++++++++++++++++++++++++++++++")
      print("+++++++++++++++++++++++++++++++++++++++++++++++++++++++++")
      print("jumlah berita  =",jumlah_berita,"----halaman =",self.hal)
      print("+++++++++++++++++++++++++++++++++++++++++++++++++++++++++")
      print("+++++++++++++++++++++++++++++++++++++++++++++++++++++++++")
      #find next page if any.
      if jumlah_berita>14:
        self.hal = self.hal+15
        next_page = 'https://index.okezone.com/bydate/channel/'+self.tanggal+'/1/'+str(self.hal)
        req = scrapy.Request(next_page, callback=self.parse)
        yield req
      else:
        print("scraping ---- Selesai Total halaman = ",self.hal)
        print("jumlah berita  =",jumlah_berita,"----halaman =",self.hal)
      
      
      
      
      
    def parse_artikel(self,response):
      
      
      judul_selector = 'div.title h1 ::text'
      waktu_selector = 'div.namerep b ::text'
      isi_selector = '#contentx p ::text'
      tag_selector = 'a.ga_Tag ::text'
            
      judul = response.css(judul_selector).get()
      waktu = response.css(waktu_selector).get()
      isi = response.css(isi_selector).getall()
      tag = '[' +','.join(response.css(tag_selector).getall())+']'
      tag = tag.replace("#","")


      isi_fix = []
      tanda=False
      for kalimat in isi:
        if tanda:
          tanda=False
          continue
        kalimatx= ''.join(findall('[a-z]',kalimat.lower()))
        logikal = kalimatx=='bacajuga'
        if logikal:
          tanda=True
          continue
        isi_fix.append(kalimat)
        
      isi = ' '.join(isi_fix)

      waktu = ''.join(findall('\d{2}\s+[a-zA-Z]+\s+\d{4}\s+\d{2}:\d{2}',waktu))

      beda = {
        'Januari':'January',
        'Februari':'February',
        'Maret':'March',
        'Mei':'May',
        'Juni':'June',
        'Juli':'July',
        'Agustus':'August',
        'Oktober':'October',
        'Desember':'December'
      }
      bulan = ''.join(findall('[a-zA-Z]+',waktu))
      
      try:
        waktu = waktu.replace(bulan,beda[bulan])
      except:
        print('already same')
      
      waktu = datetime.strptime(waktu,'%d %B %Y %H:%M')


      
      # masukkan ke item pipeline
      item = BeritaItem()
      item['tanggal'] = waktu
      item['sumber'] = 'Okezone'
      item['judul'] = judul
      item['isi_artikel'] = isi
      item['tag']=tag
      yield item