# isi file scraper kompas

# -*- coding: utf-8 -*-
"""scraper_kompas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10b2Q7XpYpERjQzDlWI1XhTda5LVYUbPU
"""

import scrapy
from re import findall 
from datetime import datetime,timedelta
import sys
from berita.items import BeritaItem
from berita.kirim_notif import kirim_notif
class Bisnis_spider(scrapy.Spider):
    #tanggal = "2020-3-19"
    name = "bisnis_spider"
    download_delay = 0.3
    tanggal=''
    costum_settings = {
      'LOG_LEVEL':'ERROR'
    }
    dropped_count = 0
    total_scraped = 0
    hal = 0
    def __init__(self,tanggal='',*args,**kwargs):
      super(Bisnis_spider, self).__init__(*args, **kwargs)
      try:
        tanggal = datetime.strptime(tanggal,"%Y-%m-%d")
        self.tanggal=datetime.strftime(tanggal,'%d+%B+%Y')
      except:
        kemarin = (datetime.now() - timedelta(1))
        self.tanggal=datetime.strftime(kemarin,'%d+%B+%Y')
            
      self.start_urls = [(
        'https://www.bisnis.com/index/page/?c=0&d='+
        self.tanggal+'&d='+
        self.tanggal+
        '&per_page=1')]

    def parse(self,response):
      konten_selektor = '.row.mb-30'
      #menghitung jumlah berita di halaman
      jumlah_berita = 0
      for konten in response.css(konten_selektor):
        #crawl on each url 
        link_selector = 'a ::attr(href)'
        link = konten.css(link_selector).extract_first()
        jumlah_berita = jumlah_berita +1
        self.total_scraped += 1
        req = scrapy.Request(link, callback=self.parse_artikel)
        yield req
      #find next page if any.
      if jumlah_berita>14:
        self.hal = self.hal+1
        next_page = (
        'https://www.bisnis.com/index/page/?c=0&d='+
        self.tanggal+'&d='+
        self.tanggal+
        '&per_page='+str(self.hal))
        req = scrapy.Request(next_page, callback=self.parse)
        yield req
      else:
        if self.total_scraped//self.dropped_count >2:
          kirim_notif()
        print("scraping ---- Selesai Total halaman = ",self.hal)
        print("jumlah berita  =",jumlah_berita,"----halaman =",self.hal)
      
      
      
      
      
    def parse_artikel(self,response):
      
      
      judul_selector = '.title-only ::text'
      waktu_selector = 'div.author span ::text'
      isi_selector = '.description p ::text'
      tag_selector = '.topik a ::text'
     
      judul = response.css(judul_selector).get()
      waktu = response.css(waktu_selector).get()
      isi = response.css(isi_selector).getall()
      tag = '[' +','.join(response.css(tag_selector).getall())+']'
      tag = tag.replace("#","")


      isi_fix = []
      tanda=False
      for kalimat in isi:
        if tanda:
          tanda=False
          continue
        kalimatx= ''.join(findall('[a-z]',kalimat.lower()))
        logikal = kalimatx=='bacajuga'
        if logikal:
          tanda=True
          continue
        isi_fix.append(kalimat)
        
      isi = ' '.join(isi_fix)

      waktu = ''.join(findall('\d{2}\s+[a-zA-Z]+\s+\d{4}',waktu))

      beda = {
        'Januari':'January',
        'Februari':'February',
        'Maret':'March',
        'Mei':'May',
        'Juni':'June',
        'Juli':'July',
        'Agustus':'August',
        'Oktober':'October',
        'Desember':'December'
      }
      bulan = ''.join(findall('[a-zA-Z]+',waktu))
      
      try:
        waktu = waktu.replace(bulan,beda[bulan])
      except:
        print('already same')
      
      waktu = datetime.strptime(waktu,'%d %B %Y')


      
      # masukkan ke item pipeline
      item = BeritaItem()
      item['waktu'] = waktu
      item['sumber'] = 'Bisnis'
      item['judul'] = judul
      item['isi_artikel'] = isi
      item['tag']=tag
      yield item