# isi file scraper kompas

# -*- coding: utf-8 -*-
"""scraper_kompas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10b2Q7XpYpERjQzDlWI1XhTda5LVYUbPU
"""

import scrapy
try:
    from urllib.parse import urljoin
except ImportError:
     from urlparse import urljoin
from re import search
from datetime import date
from berita.items import BeritaItem
from scrapy.loader import ItemLoader
import mysql.connector as MySQLdb


class KompasScraper(scrapy.Spider):
    name = "kompas_spider"
    sekarang = date.today()
    tanggal = sekarang.strftime("%Y-%m-%d")
    start_urls = [('https://indeks.kompas.com/?site=all&date=' + tanggal)]
    ihal = 2

    host = 'localhost'
    user = 'root'
    password = ''
    db = 'phoenix'

    def __init__(self):
        self.connection = MySQLdb.connect(
            host=self.host,
            user=self.user,
            passwd=self.password,
            database=self.db
        )
        self.cursor = self.connection.cursor()

    #def insert(self, query, params):


    def __del__(self):
        self.connection.close()

    @staticmethod
    def clean(s):
        ax = ""
        for a in s.split(" "):
            try:
                ax = ax + " " + search(r'\w+', a).group()
            except AttributeError:
                continue
        return ax

    def parse(self, response):
        konten_selektor = 'div.article__list.clearfix'
        print(response)
        for konten in response.css(konten_selektor):
            link_selector = 'a.article__link ::attr(href)'
            link = konten.css(link_selector).extract_first() + "?page=all"

            url = urljoin(response.url, link)

            yield scrapy.Request(url, callback=self.parse_artikel)

        nps = "div.paging__item a.paging__link.paging__link--next ::attr(href)"
        apasih = response.css(nps).extract()
        print(self.ihal, len(apasih))
        print(apasih)
        apasih2 = response.css("div.paging__item a.paging__link.paging__link--next ::text").extract()[-1]
        cek = (apasih2 == 'Next')
        print("asu443 " + apasih2)
        while cek:
            self.ihal = self.ihal + 1
            request = scrapy.Request(url=apasih[-1])
            yield request
        spider.crawler.engine.close_spider(self, reason='finished')

    def parse_artikel(self, response):
        penulis_selector = 'div#penulis ::text'
        judul_selector = 'h1.read__title ::text'
        tanggal_selector = 'div.read__time ::text'
        isi_selector = 'div.read__content ::text'

        penulis = ' '.join(response.css(penulis_selector).getall())
        judul = ' '.join(response.css(judul_selector).getall())
        tanggal = response.css(tanggal_selector).extract_first()
        isi = ' '.join(response.css(isi_selector).getall())
        isi = ''.join(isi)
        tanggal = ''.join(tanggal)
        print(isi)

        try:
            tanggal = search(r'\d{2}[/]\d{2}[/]\d{4}',tanggal).group()
        except Exception as ex :
            print(ex)

        try:
            penulis = penulis[9:]
        except:
            penulis = "Tanpa Nama"

        # penulis = self.clean(penulis)
        ##judul = self.clean(judul)
        # isi = self.clean(isi)

        berita = ItemLoader(item=BeritaItem())
        berita.add_value('judul', judul)
        berita.add_value('penulis', penulis)
        berita.add_value('isi_artikel', isi)
        berita.add_value('tanggal', tanggal)

        yield berita.load_item()
