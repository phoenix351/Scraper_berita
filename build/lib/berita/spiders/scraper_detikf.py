# isi file scraper detik finance

# -*- coding: utf-8 -*-
"""scraper_republika.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10b2Q7XpYpERjQzDlWI1XhTda5LVYUbPU
"""

import scrapy
try:
    from urllib.parse import urljoin
except ImportError:
     from urlparse import urljoin
from re import search
from berita.items import BeritaItem

class DetikFinance_scraper(scrapy.Spider):
    name = "detikf_spider"
    tanggal = "12/18/2019"
    start_urls = [('https://finance.detik.com/indeks?date='+tanggal)]
    #ihal = 2
    @staticmethod
    def clean(s):
      ax = ""
      for a in s.split(" "):
        try :
          ax = ax + " " + search(r'\w+', a).group() 
        except AttributeError:
          continue
      return ax  
    def parse(self,response):
      konten_selektor = 'ul li'
      
      for konten in response.css(konten_selektor):
        link_selector = '.desc_idx.ml10 a ::attr(href)'
        link = konten.css(link_selector).extract_first()
        print(link)
        url = urljoin(response.url, link)
        yield scrapy.Request(url, callback=self.parse_artikel)
        
    def parse_artikel(self,response):
      
      penulis_selector = 'div.author ::text'
      judul_selector = 'div.jdl h1 ::text'
      tanggal_selector = 'div.date ::text'
      isi_selector = 'div#detikdetailtext ::text'
      penulis = ' '.join(response.css(penulis_selector).getall())
      judul = ' '.join(response.css(judul_selector).getall())
      tanggal = response.css(tanggal_selector).extract_first()
      isi = ' '.join(response.css(isi_selector).getall())
      penulis = self.clean(penulis)
      judul = self.clean(judul)
      isi = self.clean(isi)

      berita = BeritaItem()
      berita['judul'] = judul
      berita['penulis'] = penulis
      berita['isi_artikel'] = isi
      berita['tanggal'] = tanggal

      yield berita